{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predictive Analysis of Credit Card Defaults**\n",
    "\n",
    "### Objective: \n",
    "- Dataset of customers' default payments.\n",
    "- The primary goal is to predict which credit card clients are likely to default using various data mining methods.\n",
    "\n",
    "### Background: \n",
    "Traditional risk management models classify clients as either credible or not credible based on their likelihood of default. This project aims to refine this classification by identifying specific individuals who are likely to default, enhancing the precision of credit risk assessments.\n",
    "\n",
    "Target variable\n",
    "- default.payment.next.month: Default payment (1=yes, 0=no)\n",
    "\n",
    "The dataset contains the following features\n",
    "\n",
    "1. ID: ID of each client\n",
    "2. LIMIT_BAL: Amount of given credit in dollars (includes individual and family/supplementary credit\n",
    "3. SEX: Gender (1=male, 2=female)\n",
    "4. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "5. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "6. AGE: Age in years\n",
    "7. PAY_0: Repayment status in September (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "8. PAY_2: Repayment status in August (scale same as above)\n",
    "9. PAY_3: Repayment status in July, (scale same as above)\n",
    "10. PAY_4: Repayment status in June (scale same as above)\n",
    "11. PAY_5: Repayment status in May (scale same as above)\n",
    "12. PAY_6: Repayment status in April (scale same as above)\n",
    "13. BILL_AMT1: Amount of bill statement in September (dollars)  \n",
    "14. BILL_AMT2: Amount of bill statement in August (dollars)  \n",
    "15. BILL_AMT3: Amount of bill statement in July (dollars)  \n",
    "16. BILL_AMT4: Amount of bill statement in June (dollars)  \n",
    "17. BILL_AMT5: Amount of bill statement in May (dollars)  \n",
    "18. BILL_AMT6: Amount of bill statement in April (dollars)   \n",
    "19. PAY_AMT1: Amount of previous payment in September (dollars)  \n",
    "20. PAY_AMT2: Amount of previous payment in August (dollars)  \n",
    "21. PAY_AMT3: Amount of previous payment in July (dollars)   \n",
    "22. PAY_AMT4: Amount of previous payment in June (dollars)  \n",
    "23. PAY_AMT5: Amount of previous payment in May (dollars)   \n",
    "24. PAY_AMT6: Amount of previous payment in April (dollars)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading the dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   10000 non-null  int64  \n",
      " 1   SEX                         9617 non-null   float64\n",
      " 2   EDUCATION                   9617 non-null   float64\n",
      " 3   MARRIAGE                    9617 non-null   float64\n",
      " 4   AGE                         9617 non-null   float64\n",
      " 5   PAY_0                       9617 non-null   float64\n",
      " 6   PAY_2                       9617 non-null   float64\n",
      " 7   PAY_3                       9617 non-null   float64\n",
      " 8   PAY_4                       9617 non-null   float64\n",
      " 9   PAY_5                       9617 non-null   float64\n",
      " 10  PAY_6                       9642 non-null   float64\n",
      " 11  BILL_AMT1                   9642 non-null   float64\n",
      " 12  BILL_AMT2                   9642 non-null   float64\n",
      " 13  BILL_AMT3                   9642 non-null   float64\n",
      " 14  BILL_AMT4                   9642 non-null   float64\n",
      " 15  BILL_AMT5                   9632 non-null   float64\n",
      " 16  BILL_AMT6                   9632 non-null   float64\n",
      " 17  PAY_AMT1                    9632 non-null   float64\n",
      " 18  PAY_AMT2                    9632 non-null   float64\n",
      " 19  PAY_AMT3                    9990 non-null   float64\n",
      " 20  PAY_AMT4                    9990 non-null   float64\n",
      " 21  PAY_AMT5                    9710 non-null   float64\n",
      " 22  PAY_AMT6                    9710 non-null   float64\n",
      " 23  default payment next month  10000 non-null  int64  \n",
      "dtypes: float64(22), int64(2)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"assignment_data/credit_data.xlsx\")\n",
    "\n",
    "# Create a pandas dataframe contining the first 10,000 rows from the credit card dataset\n",
    "credit_df = df.head(10000)\n",
    "\n",
    "# Delete the 'ID' column\n",
    "credit_df = credit_df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Print dataframe info\n",
    "credit_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "- Numeric variables are measured on a continuous or discrete scale and support arithmetic operations (for example age, bill amount).\n",
    "- Ordinal variables represent categories with a meaningful order, but the intervals between them are not necessarily uniform (for example payment status).\n",
    "- Nominal variables represent categories without a natural order (for example gender, marital status).\n",
    "\n",
    "\n",
    "\n",
    "**Table: Classification of Features**\n",
    "\n",
    "| Variable Kind | Number of Features | Feature Names |\n",
    "|---------------|--------------------|---------------|\n",
    "| **Numeric**   | 14 | LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6 |\n",
    "| **Ordinal**   | 7  | EDUCATION, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6 |\n",
    "| **Nominal**   | 3  | SEX, MARRIAGE, default payment next month |\n",
    "\n",
    "\n",
    "\n",
    "**Description:**\n",
    "\n",
    "After removing the `ID` column, the dataset contains 24 features relevant to predicting credit card defaults.\n",
    "\n",
    "- There are 14 numeric features, such as `LIMIT_BAL` (credit limit), `AGE` and many bill and payment amounts from April to September. These are continuous variables that reflect the financial behavior of clients.\n",
    "\n",
    "- 7 features are classified as ordinal, including the six `PAY_` variables (`PAY_0`, `PAY_2` to `PAY_6`), which indicate repayment status on a monthly scale, with higher values corresponding to longer payment delays. \n",
    "    + `EDUCATION` is also treated as ordinal, as the values `1` (graduate school), `2` (university) and `3` (high school) follow a logical order. However, this feature includes categories `4` (others) and `5` – `6` (unknown) creates ambiguity, as these do not clearly fit into a ranked structure. To preserve the ordinal nature of the variable in analysis, these categories (`5` and `6`) might be grouped together as `4` (others) during preprocessing.\n",
    "\n",
    "- The remaining 3 features are nominal: `SEX`, `MARRIAGE` and `default payment next month`. These represent categorical data with no inherent order and should be encoded accordingly.\n",
    "\n",
    "This classification guides the choice of appropriate preprocessing strategies and modeling techniques, helping ensure that the data is interpreted correctly based on its underlying structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIMIT_BAL                       0\n",
       "default payment next month      0\n",
       "PAY_AMT4                       10\n",
       "PAY_AMT3                       10\n",
       "PAY_AMT5                      290\n",
       "PAY_AMT6                      290\n",
       "BILL_AMT2                     358\n",
       "BILL_AMT4                     358\n",
       "BILL_AMT3                     358\n",
       "PAY_6                         358\n",
       "BILL_AMT1                     358\n",
       "BILL_AMT5                     368\n",
       "BILL_AMT6                     368\n",
       "PAY_AMT1                      368\n",
       "PAY_AMT2                      368\n",
       "PAY_4                         383\n",
       "SEX                           383\n",
       "PAY_3                         383\n",
       "PAY_2                         383\n",
       "PAY_0                         383\n",
       "AGE                           383\n",
       "MARRIAGE                      383\n",
       "PAY_5                         383\n",
       "EDUCATION                     383\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Number of missing values for each variable\n",
    "missing_values_credit = credit_df.isnull().sum().sort_values()\n",
    "missing_values_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The output shows that several variables in the dataset contain missing values, which need to be addressed before training model:\n",
    "\n",
    "- Repayment status features (`PAY_0`, `PAY_2`, `PAY_3`, `PAY_4`, `PAY_5`, `PAY_6`) have between 358 and 383 missing entries. Since these variables help us understand how timely clients are with their payments and are ranked in severity, therefore, they should be imputed carefully, ideally in a way that maintains their order.\n",
    "\n",
    "- Bill statement amounts features (`BILL_AMT1` to `BILL_AMT6`) have moderate missing entries, ranging from 358 to 368 values. These are continuous numeric variables and are typically imputed using the mean.\n",
    "\n",
    "- Payment amount features (`PAY_AMT1` to `PAY_AMT6`) show varying levels of missing entries, with some as low as 10 and others up to 368. These numeric variables should be imputed consistently to avoid having bias.\n",
    "\n",
    "- Demographic and categorical features such as `SEX`, `AGE`, `MARRIAGE` and `EDUCATION` each have 383 missing entries. Given their nominal or ordinal nature, mode imputation would be the most appropriate strategy.\n",
    "\n",
    "- It is also worth noting that both the target variable (`default payment next month`) and a key numeric predictor `LIMIT_BAL`, have no missing values. This ensures the target is fully usable for supervised learning and that one of the most informative features is complete.\n",
    "\n",
    "In summary, the missing values are non-trivial and span across multiple important predictors. Addressing them properly will help ensure the model is both reliable and accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Cleaning data and dealing with categorical features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for the clean dataset to preserve the original data\n",
    "cleaned_credit_df = credit_df.copy()\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_col = [\n",
    "    'LIMIT_BAL', 'AGE', \n",
    "    'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "# Impute numeric columns with mean\n",
    "for col in numeric_col:\n",
    "    if cleaned_credit_df[col].isnull().sum() > 0:\n",
    "        cleaned_credit_df[col].fillna(cleaned_credit_df[col].mean(), inplace=True)\n",
    "\n",
    "\n",
    "# Define nominal/ordinal columns \n",
    "cat_col = [\n",
    "    'SEX', 'EDUCATION', 'MARRIAGE',\n",
    "    'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "# Impute nominal/ordinal columns with mode\n",
    "for col in cat_col:\n",
    "    cleaned_credit_df[col].fillna(cleaned_credit_df[col].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation is the process of replacing missing values with estimated ones based on observed data. This step ensures the dataset is complete and suitable for model training, preventing errors or potential biases that could arise from null values.\n",
    "\n",
    "In this assignment, I applied two primary strategies based on feature type:\n",
    "\n",
    "- Mean imputation was used for all numeric variables (for example `AGE`, `BILL_AMT` and `PAY_AMT`), as these features are continuous and replacing missing values with the average is statistically appropriate and maintains the overall distribution.\n",
    "\n",
    "- Mode imputation was used for nominal and ordinal variables (for example `SEX`, `EDUCATION`, `MARRIAGE` and repayment status variables like `PAY_0`, `PAY_2`, etc), since these features represent discrete groups. Filling in the most frequent category helps preserve the variable’s interpretability and structure.\n",
    "\n",
    "These decisions were guided by the earlier features classification (see Q2), ensuring that the imputation approach aligned with the nature of each variable. This structured strategy supports both the integrity of the dataset and the performance of the predictive models built on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts of the SEX column:\n",
      " SEX\n",
      "2.0    6162\n",
      "1.0    3838\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print value counts of the 'SEX' column\n",
    "print(\"Value counts of the SEX column:\\n\", cleaned_credit_df['SEX'].value_counts())\n",
    "\n",
    "# Convert 'SEX' column to integer\n",
    "cleaned_credit_df['SEX'] = cleaned_credit_df['SEX'].astype(int)\n",
    "\n",
    "# Apply `get_dummies()` to the 'SEX' column\n",
    "sex_dummies = pd.get_dummies(cleaned_credit_df['SEX'], prefix='SEX', drop_first=True, dtype=int)\n",
    "\n",
    "# Rename SEX_2 to SEX_FEMALE and add it to the dataframe\n",
    "cleaned_credit_df['SEX_FEMALE'] = sex_dummies['SEX_2']\n",
    "\n",
    "# Drop the original 'SEX' column\n",
    "cleaned_credit_df.drop(columns=['SEX'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The `value_counts()` function was first used to examine the distribution of the `'SEX'` variable, showing:\n",
    "- `2.0` = Female with 6,162 clients  \n",
    "- `1.0` = Male with 3,838 clients  \n",
    "\n",
    "This gives a clear understanding of the gender breakdown in the dataset.\n",
    "\n",
    "Next, the `'SEX'` column was converted to integers using `.astype(int)` to ensure compatibility with `pd.get_dummies()`.\n",
    "\n",
    "I then applied `pd.get_dummies()` with `drop_first=True`, which avoids multicollinearity by generating only one dummy variable. Since the original values are `1 = male` and `2 = female`, this created a column named `'SEX_2'`, corresponding to clients who are female. To match the assignment instructions and clearly reflect the category it represents, I renamed `SEX_2` to `SEX_FEMALE`.\n",
    "\n",
    "Meaning of the new variable `SEX_FEMALE`:\n",
    "- `SEX_FEMALE = 1` means the client is female\n",
    "- `SEX_FEMALE = 0` means the client is male\n",
    "\n",
    "\n",
    "This transformation ensures the gender feature is in a numeric format suitable for machine learning models, while retaining its interpretability. I also used `dtype=int` to make sure the dummy values are stored as `0` and `1`, rather than Boolean values.\n",
    "\n",
    "Finally, I dropped the original `'SEX'` column to prevent redundancy, as its information is now fully captured in `'SEX_FEMALE'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts of the 'MARRIAGE' column:\n",
      " MARRIAGE\n",
      "2.0    5518\n",
      "1.0    4380\n",
      "3.0      82\n",
      "0.0      20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print value counts of the 'MARRIAGE' column\n",
    "print(\"Value counts of the 'MARRIAGE' column:\\n\", cleaned_credit_df['MARRIAGE'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "According to the dataset definition, the `MARRIAGE` variable should only contain three valid categories:\n",
    "\n",
    "- `1` = Married\n",
    "- `2` = Single\n",
    "- `3` = Others\n",
    "\n",
    "However, the `value_counts()` output shows that there are also 20 entries are coded as `0`, which is not defined in the original variable description. This suggests a discrepancy between the dataset and its documentation, suggesting a data quality issue. These undefined values may result from input errors or inconsistent coding and must be addressed during preprocessing to prevents potential bias or misleading during model training.\n",
    "\n",
    "Most clients are either single (5,518) or married (4,380), while only 82 are classified as “others.” Since the `0` values are undefined and fewer than category `3`, a practical solution is to reassign them to category `3` (Others) to preserve all data without introducing an invalid class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply `get_dummies()` to 'MARRIAGE' feature and add dummy variables 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHER' to `df`  \n",
    "\n",
    "# Reassign invalid values (0) to 3 = 'Other'\n",
    "cleaned_credit_df['MARRIAGE'] = cleaned_credit_df['MARRIAGE'].replace({0: 3})\n",
    "\n",
    "# Convert MARRIAGE column to integers \n",
    "cleaned_credit_df['MARRIAGE'] = cleaned_credit_df['MARRIAGE'].astype(int)\n",
    "\n",
    "# Apply `get_dummies()` to 'MARRIAGE' feature\n",
    "marriage_dummies = pd.get_dummies(cleaned_credit_df['MARRIAGE'], prefix='MARRIAGE', dtype=int)\n",
    "\n",
    "# Rename dummy columns to match assignment instructions\n",
    "marriage_dummies.rename(\n",
    "    columns={\n",
    "        'MARRIAGE_1': 'MARRIAGE_MARRIED',\n",
    "        'MARRIAGE_2': 'MARRIAGE_SINGLE',\n",
    "        'MARRIAGE_3': 'MARRIAGE_OTHER'},\n",
    "    inplace=True)\n",
    "\n",
    "# Concatenate dummy variables to the main dataframe\n",
    "cleaned_credit_df = pd.concat([cleaned_credit_df, marriage_dummies], axis=1)\n",
    "\n",
    "# Drop the original 'MARRIAGE' column\n",
    "cleaned_credit_df.drop(columns=['MARRIAGE'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The `value_counts()` function for the `MARRIAGE` column showed an unexpected value `0`, which does not appear in the dataset definition. According to the dataset:\n",
    "\n",
    "- `1` is Married\n",
    "- `2` is Single\n",
    "- `3` is Others\n",
    "\n",
    "The presence of `0` is likely a data entry error or inconsistency. Since the instructionss state not to delete observations or treat anomalies as missing, I reassigned all instances of `0` to category `3` (Others). This approach preserves data integrity and ensures potentially useful patterns are not lost. It is also semantically consistent since `0` clearly does not represent a meaningful marital status on its own, so grouping it with “Others” avoids introducing noise or misinterpretation into the model.\n",
    "\n",
    "To prepare the variable for modeling, I first converted the `MARRIAGE` column to integer type using `.astype(int)`. This step ensures that the category values are clean integers, which prevents column names like `MARRIAGE_1.0` from appearing during dummy encoding.\n",
    "\n",
    "Next, I applied `pd.get_dummies()` to the `MARRIAGE` column to convert it into binary variables. The `dtype=int` ensures that the resulting values are stored as integers (`0` and `1`), which is preferred by most machine learning algorithms over boolean types.\n",
    "\n",
    "By default, the resulting dummy columns were named numerically (`MARRIAGE_1`, `MARRIAGE_2`, `MARRIAGE_3`), reflecting the original numeric codes. To ensure clarity and match the assignment instructions along with the dataset definitions (where `1` = married, `2` = single and `3` = others), I renamed the columns using `.rename()` as follows:\n",
    "\n",
    "- `MARRIAGE_1` to  `MARRIAGE_MARRIED`  \n",
    "- `MARRIAGE_2` to `MARRIAGE_SINGLE`  \n",
    "- `MARRIAGE_3` to `MARRIAGE_OTHER`  \n",
    "\n",
    "Each of these variables follows the binary format (`0` or `1`) and represents:\n",
    "\n",
    "- `MARRIAGE_MARRIED` = 1 if the client is married, 0 otherwise  \n",
    "- `MARRIAGE_SINGLE` = 1 if the client is single, 0 otherwise  \n",
    "- `MARRIAGE_OTHER` = 1 if the client belongs to the \"other\" category (including any recoded `0`), 0 otherwise  \n",
    "\n",
    "Finally, I concatenated the renamed dummy columns back into the original DataFrame and dropped the original `MARRIAGE` column to avoid redundancy. This preprocessing step ensures that the feature is clean, encoded and ready for use in machine learning models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values {0, 5, 6} to the value 4 In the column 'EDUCATION'\n",
    "\n",
    "# Convert values 0, 5, and 6 in 'EDUCATION' to 4\n",
    "cleaned_credit_df['EDUCATION'] = cleaned_credit_df['EDUCATION'].replace({0: 4, 5: 4, 6: 4})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing X and y arrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Create a numpy array `y` from the first 7,000 observations of `default payment next month` column from `df`\n",
    "- Create a numpy array `X`  from the first 7,000 observations of all the remaining variables in `df` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a numpy array `y` from the first 7,000 observations of `default payment next month` column from `df`\n",
    "y = cleaned_credit_df['default payment next month'].iloc[:7000].to_numpy()\n",
    "\n",
    "# Create a numpy array `X`  from the first 7,000 observations of all the remaining variables in `df`\n",
    "X = cleaned_credit_df.drop(columns=['default payment next month']).iloc[:7000].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into 70% train and 30% test datasets with stratification and random_state=31\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=31, stratify=y)\n",
    "\n",
    "# Standardise the data to mean zero and variance one using an approapriate `sklearn` library\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Models and Interpretation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (Logistic Regression): 0.821\n",
      "Test Accuracy (Logistic Regression): 0.805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a linear classifier (Logistic Regression) using standardised data\n",
    "lr = LogisticRegression(random_state=31)\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Compute training and test accuracies\n",
    "train_acc = lr.score(X_train_std, y_train)\n",
    "test_acc = lr.score(X_test_std, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Accuracy (Logistic Regression): {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy (Logistic Regression): {test_acc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (KNN): 0.843\n",
      "Test Accuracy (KNN): 0.780\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train nonlinear classifier (K-Nearest Neighbors) on the same dataset\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)  \n",
    "knn_clf.fit(X_train_std, y_train)\n",
    "\n",
    "# Compute training and test accuracies\n",
    "train_acc_knn = knn_clf.score(X_train_std, y_train)\n",
    "test_acc_knn = knn_clf.score(X_test_std, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Accuracy (KNN): {train_acc_knn:.3f}\")\n",
    "print(f\"Test Accuracy (KNN): {test_acc_knn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1) Results obtained from the two classifiers - Comparision**\n",
    "\n",
    "- The logistic regression model achieved a training accuracy of 0.821 and a test accuracy of 0.805. In contrast, the K-Nearest Neighbors (KNN) model reached a higher training accuracy of 0.843, but a lower test accuracy of 0.780.\n",
    "\n",
    "- While both models performed reasonably well, logistic regression demonstrated a better generalisation to unseen data. The small gap between its training and test scores suggests the model is stable and not overfitting.\n",
    "\n",
    "- On the other hand, KNN shows signs of overfitting. Its higher training accuracy, along with a noticeable drop in test performance, suggests that the model may be fitting too closely to the training data. This behaviour is typical of memory-based, nonlinear models such as KNN, which often capture local patterns that do not generalise well to unseen data.\n",
    "\n",
    "**2) Recommendation model**\n",
    "\n",
    "- Based on the observed performance, I would recommend logistic regression. Despite its slightly lower training accuracy, it achieved better performance on the test set, indicating more reliable predictions on new data — a key goal in credit risk modeling.\n",
    "\n",
    "- Moreover, logistic regression is also highly interpretable, which is valuable in risk assessments and in financial contexts where decisions must be explainable and transparent to stakeholders. Its coefficients can provide insights into which variables most strongly influence default risk, which help support decision-making.\n",
    "\n",
    "- KNN may still be useful in specific scenarios where the goal is to capture nonlinear patterns, but in this case, its performance suggests it is less suited for the given dataset without further tuning or feature engineering.\n",
    "\n",
    "- Finally, this project goes beyond traditional binary classification, which simply labels clients as credible or not. Instead, it focuses on identifying exactly who is most likely to default, allowing for more targeted risk management. Among the models tested, logistic regression offered the best balance between performance, interpretability and robustness, making it highly suitable for real-world financial applications, where both accuracy and explainability are essential.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
